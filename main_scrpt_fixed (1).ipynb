{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GyBHNd9JDp2"
      },
      "source": [
        "## Установка зависимостей и загрузка библиотек"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Устанавливаем зависимости и подключаем библиотеки для RAG-Telegram-бота: векторный поиск и хранилище (Qdrant), эмбеддинги текста (Sentence Transformers), загрузка и чанкинг документов (LlamaIndex), генерация ответов LLM (OpenAI API) и интерфейс Telegram-бота (pyTelegramBotAPI)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmEq1EAmHzvU",
        "outputId": "7db56117-60bb-440c-c2aa-ee3f623e4a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.2/377.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q qdrant-client==1.16.2 sentence-transformers llama-index openai\n",
        "!pip install -q pyTelegramBotAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xla48X8CHzr0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import requests\n",
        "import telebot\n",
        "import threading\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from qdrant_client import QdrantClient, models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from openai import OpenAI\n",
        "\n",
        "# Конфигурация из config.py (секреты из .env или переменных окружения; в Colab — через Secrets)\n",
        "import config\n",
        "openai_client = OpenAI(api_key=config.OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mzopqBZKGdG"
      },
      "source": [
        "## Инициализация Qdrant и конфигурация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Код настраивает подключение к Qdrant и проверяет наличие векторной коллекции. Если коллекция не существует, он создаёт её с заданной размерностью векторов и косинусной метрикой, подготавливая хранилище для эмбеддингов и последующего векторного поиска."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEcW0IaiHzpP",
        "outputId": "85a1657c-f04b-41ac-f18d-150c54fa8965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection: career_levels_bge_v2\n"
          ]
        }
      ],
      "source": [
        "# Конфигурация из config\n",
        "QDRANT_URL = config.QDRANT_URL\n",
        "QDRANT_API_KEY = config.QDRANT_API_KEY\n",
        "collection_name = config.COLLECTION_NAME\n",
        "embed_model_name = config.EMBED_MODEL_NAME\n",
        "vector_size = config.VECTOR_SIZE\n",
        "# Qdrant клиент\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_URL,\n",
        "    api_key=QDRANT_API_KEY\n",
        ")\n",
        "\n",
        "# Создание коллекции если не существует\n",
        "existing = [c.name for c in qdrant.get_collections().collections]\n",
        "if collection_name not in existing:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE),\n",
        "    )\n",
        "print(\"Collection:\", collection_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8lHMfSsNibJ"
      },
      "source": [
        "## Загрузка и обработка документа"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Загрузка и обработка данных\n",
        "- Загрузка документа text2.txt\n",
        "- Разделение текста на чанки (chunk_size=256, overlap=80)\n",
        "- Подготовка текстовых фрагментов для индексации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvk30DiSHzkG",
        "outputId": "3135d4e9-dfa2-4da4-efd5-9217c40cbaf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded documents: 1\n",
            "Бэкенд-разработчик отвечает за серверную часть продукта: бизнес-логику, работу с данными, интеграции, безопасность и надежность систем. Его работа обеспечивает стабильность, масштабируемость и предсказуемость пользовательского опыта, даже если конечный пользователь напря\n",
            "Chunks (nodes): 56\n",
            "Бэкенд-разработчик отвечает за серверную часть продукта: бизнес-логику, работу с данными, интеграции, безопасность и надежность систем. Его работа обеспечивает стабильность, масштабируемость и предсказуемость пользовательского опыта, даже если конечный пользователь напря\n"
          ]
        }
      ],
      "source": [
        "# Загрузка документа\n",
        "documents = SimpleDirectoryReader(input_files=[\"text2.txt\"]).load_data()\n",
        "print(\"Loaded documents:\", len(documents))\n",
        "print(documents[0].text[:300])\n",
        "\n",
        "# Чанкинг\n",
        "splitter = SentenceSplitter(chunk_size=config.CHUNK_SIZE, chunk_overlap=config.CHUNK_OVERLAP)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "texts = [n.text for n in nodes]\n",
        "print(\"Chunks (nodes):\", len(texts))\n",
        "print(texts[0][:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMKAPDghOBRB"
      },
      "source": [
        "## Создание эмбеддингов и индексация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Код преобразует текстовые чанки в эмбеддинги с помощью модели SentenceTransformer, нормализуя векторы для корректного косинусного поиска. Затем для каждого чанка формируется точка с вектором и метаданными. Эти точки загружаются в Qdrant с помощью upsert, создавая или обновляя векторный индекс. В результате данные становятся доступными для быстрого семантического поиска и использования в RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522,
          "referenced_widgets": [
            "f75611dfb0f14bcc84edf67bc996ec5e",
            "ab2ad69165ee4274bdb3d55bed08426e",
            "2457cbf3daf5440780587cd727dd1b15",
            "8f989c77fe5143a58e0ea09a21c76e3e",
            "d3be5914d35b434d9bc45372de680aa8",
            "6a194a478145477ba5f11e93df77be13",
            "3623b3e88a5c49729b145282dbe4a342",
            "5ea7ee8709f3490287021f2c350d9315",
            "d2d32f8e0ace4d75bf4c7cccb7fac568",
            "4233b347009b47989d10b3b581fd8ced",
            "a3420ad8d9cb4223b9353c311dfe8284",
            "7331282d10a04ca29b227fa79ec26da8",
            "6874cebc703345dfa86c772f9cd9e274",
            "1b0a7dd5f7534abe9769443456fca033",
            "05a56bb39c8242afb731480f52164169",
            "f391e52974ed467fb4e01c39d90f37bc",
            "f85c8a9b50cc4548801ccb55fb742d09",
            "b158a6f586d84fb99d000ade8e61e77b",
            "ed2a96fbfbfb436888e21060426dd093",
            "186a3639bea847dcb24d2c1c31313872",
            "3f140dddf2784b76a14eaf935fb4b2f6",
            "63937a9ba6de4b6d9032eb0d0e6b529a",
            "e0ed2807d08842f68b1ab3da80cb5d59",
            "a7800ad518994c85984b0a7eba57ba97",
            "69c5ea4c98be4c2f837faddce11b149f",
            "814977f57bfa45feb9181258ef9aacbc",
            "b012dd0801144164ab39f4f5c54d29f8",
            "535c3919634242a9b683fa927f13a3d6",
            "f967139304694813a171ccced1a56b83",
            "c12b8586925242eaa5a76c204e6d99fe",
            "7c149604e2ec489684337c0a7e4cfdd0",
            "f036aff798064eb89b0fb3a47a7c2a49",
            "3ae3039886f944d58479d2289ee68ab4",
            "aced26f085254a20ba788b0594297242",
            "eb95e1d8ff4d4faea58dbdc25abeb754",
            "af204971388c490288a95bfd4f730007",
            "3cd67f439d004c4b802d3d1929b8d51f",
            "caa7cf9b23024188aed4fb8db157617f",
            "b76e875e106d4a3ab3e4316d19992b9f",
            "637ca6d9f345497bbe61afc63c6a0a47",
            "612b77225c634cd9be014b384509abfc",
            "3f07494fcaaf4d4d86e33307f09f9904",
            "64a16e7ebe314c52a1ec67579a74ad53",
            "2475160e425f45f3b1ee37fd878ebe6d",
            "3272a36f303e4d298912cb7a97cf4c23",
            "73ce0a5e3bb046c4823d1e27f1666579",
            "cdc18c5687234248b7ac537d8c2189e2",
            "7d79751030e6426c9904c29b87db4b59",
            "18d2c227e7a24262a3dabad6b424ebfb",
            "3c62debff0de422c9a68695d975a8a2e",
            "028f4dbb47ea4f26927b172182e9da79",
            "2a4348a928f64b8aa48daa6289b2b595",
            "27d020a498a24b76b812c4e6b1bb16b8",
            "133d02e6e64e4bd68f53381da9fe6bfa",
            "62b8bff3ea974f568a9bda700928536e",
            "5a6e4fee13c44c3fb83f366c53abe828",
            "c26c7123f44e4ddba0e85f228c84b640",
            "3689685f4e784506b0f51977f7492bd7",
            "13d435974b04417ea1d3f2fbe3b97984",
            "10f1636cc4f948c6b683f0ed6eef5925",
            "78ef1d630dd04dfab1838b69d4fea453",
            "b862673e7dec4b81b21403bc9dd59720",
            "0678396a972f4d5984f7d94624b9a7a9",
            "3279ec60ff07452e9fe6de526ef92585",
            "354e1c9870fa4f9b8c1bc107959a2bef",
            "dac114db87d841fc8d5d895e848e9c12",
            "342f74662b6d4a27bba454f1ba72cf8f",
            "be63a2dd52264cd0a6b58a8cf2da9de1",
            "43f60bc1c1bd4153a2279a638c41ae18",
            "e4e57d52026e442681b1e767ebc4b0cd",
            "f76ffb23214242908904cf1ef6e4bdb5",
            "1746a928f9dd445db39bb4747b260af2",
            "dd16cc3bbf7b4d8a8f32d99337a65b0e",
            "1a1e67f473214bb39ee34e472b8f9cfc",
            "dba41bf22b4c474ba817c7263ba53feb",
            "a5f26b74d0144720898a78df0421d4fd",
            "15bd53088e994b93aee92e5f58c62a79",
            "05b91c71f3f6464b9191926d85abebe7",
            "845a52004922407ca439c16fb65df1f0",
            "d38e4ed65dbd43cc969d9843c6af6802",
            "9a17c2af33ef4b62ac07e72c8f60a652",
            "4b9d361182da4318a3be346b0b08638e",
            "5d99451e112e4aa495b83a8102fec806",
            "33d832e0a4284e78b8230afff71530b1",
            "74e4307a66bc4e8d92cfc07dc7c46f88",
            "c53edd00c2c84f039707ba9965ad9178",
            "c5a73ad5f537417a94077651f2b712fe",
            "e4e714dd05fa44b6b5085c8c7772c8c6",
            "3c56f7de76024eb88e939bd17e69b2de",
            "2c5fbf8b8d8e470a95b9f2d28075bbdd",
            "bd51510120494c06a9da55fafa8a031f",
            "d6060e56ed2a4d2390cae09a178f456f",
            "3462193cef3e4975960391d00eb821cf",
            "f0f4c3ff33d049c3911a220f52b955c0",
            "ab72646d49604325a97c825bc4ee2c10",
            "0bac04d70dba410e87e90c5dce90aa90",
            "41c5a93effaa4b43b4a02bb8a3864a66",
            "f6f14043b6874c3eb5f91e26395aea86",
            "10c9ee54eb2a4a30aed16e5458a5f6fd",
            "5a24d00791724328a73a5e09f97558af",
            "ac8eadb9adc043eb86b2563995830b8b",
            "8ba4ede4afba401ea6547fdc6b847921",
            "027e369361a84bc98e8839beb41962a1",
            "6ce73cb7a8004e86b6d039675d9ba27f",
            "18500cbd75b8410689c9e174a8c611ae",
            "e9fb34e86c1d46f6b724bac8379d6717",
            "0d5f693535c545f5897de7f5ecf7dcd5",
            "303ca9f5d37e4e45bc679af752fd1ad2",
            "3b807357fb4e4e8cbdb919f821918e10",
            "d01a3aa26b844c72af1baa3ed3b2f4da",
            "49305236f9544558a159bb80ac0b0d98",
            "dd96c87816db4ea0b97f6f7404cb46e4",
            "a650079c181843a9b2f737f07f81c6b7",
            "20b2df69814e45dd9533d97c8fe4704e",
            "25cbbf2763e94d868b4b932d959fb244",
            "75d24a715fc64f9ea34f51b968ed097d",
            "f990a1b25fd149aeba3df16ae88c2f41",
            "fa253d71e6bb4b50bf96844bf93111c5",
            "57f757cc6ccc460ea08b993a0c726540",
            "8db270a15cba44848f75ebde0fd6705b",
            "b265de1f82404359bb4ba7e398b78b5f",
            "d8289cb7c83944da9c27fe1a7b8a4465",
            "37ee4f57f5ef474faea855733abdedc7",
            "a5debd9f13944cf6a881fca060e53049",
            "7d1912c0f7ea4ba29ad1d41ff4d54815",
            "3faa4565d95a4325981fc7723e1f3b46",
            "3006445c8c99489e8a0a31a5adf02715",
            "563c1172dc384faabf27460b5a3243aa",
            "ec6487a8c3af415eb5a53f7cb8559405",
            "e3ce237a71c24ba8bec325eddb40c2bd",
            "582c91ee01f04c23b43c743ab3387315",
            "fae51d03f84a4cda89d84929289b976e"
          ]
        },
        "id": "A64MQTtgHzh7",
        "outputId": "095c461f-429c-48fc-a4dd-573cadf30064"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f75611dfb0f14bcc84edf67bc996ec5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7331282d10a04ca29b227fa79ec26da8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0ed2807d08842f68b1ab3da80cb5d59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aced26f085254a20ba788b0594297242",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3272a36f303e4d298912cb7a97cf4c23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a6e4fee13c44c3fb83f366c53abe828",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "342f74662b6d4a27bba454f1ba72cf8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05b91c71f3f6464b9191926d85abebe7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c56f7de76024eb88e939bd17e69b2de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a24d00791724328a73a5e09f97558af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49305236f9544558a159bb80ac0b0d98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8289cb7c83944da9c27fe1a7b8a4465",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upserted: 56\n"
          ]
        }
      ],
      "source": [
        "# Эмбеддинги\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "vectors = embedder.encode(\n",
        "    texts,\n",
        "    normalize_embeddings=True,\n",
        "    batch_size=64,\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "\n",
        "# Подготовка точек для Qdrant\n",
        "points = [\n",
        "    models.PointStruct(\n",
        "        id=idx,\n",
        "        vector=vectors[idx].tolist(),\n",
        "        payload={\"text\": texts[idx], \"chunk_id\": idx, \"source\": \"text.txt\"},\n",
        "    )\n",
        "    for idx in range(len(texts))\n",
        "]\n",
        "\n",
        "# Загрузка в Qdrant\n",
        "qdrant.upsert(collection_name=collection_name, points=points)\n",
        "print(\"Upserted:\", len(points))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFnrQxMbOQDu"
      },
      "source": [
        "## Функции ретривера и генерации ответов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сначала функция retrieve преобразует вопрос в эмбеддинг и выполняет векторный поиск в Qdrant, возвращая наиболее релевантные текстовые чанки. Затем generate_rag_answer собирает найденные чанки в контекст и передаёт его в LLM (OpenAI), которая формирует ответ, строго опираясь на этот контекст. Функция answer объединяет оба шага и возвращает пользователю финальный ответ либо сообщение об отсутствии релевантных знаний в базе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmG7JyeRHze3"
      },
      "outputs": [],
      "source": [
        "def retrieve(query: str, top_k: int = 10):\n",
        "    \"\"\"Функция поиска релевантных чанков\"\"\"\n",
        "    qvec = embedder.encode([query], normalize_embeddings=True)[0].tolist()\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    if QDRANT_API_KEY:\n",
        "        headers[\"api-key\"] = QDRANT_API_KEY\n",
        "\n",
        "    payload = {\n",
        "        \"vector\": qvec,\n",
        "        \"limit\": top_k,\n",
        "        \"with_payload\": True,\n",
        "        \"with_vector\": False,\n",
        "    }\n",
        "\n",
        "    r = requests.post(\n",
        "        f\"{QDRANT_URL}/collections/{collection_name}/points/search\",\n",
        "        json=payload,\n",
        "        headers=headers,\n",
        "        timeout=60,\n",
        "    )\n",
        "    r.raise_for_status()\n",
        "    return r.json()[\"result\"]\n",
        "\n",
        "hits = retrieve(\"Какие навыки нужно развивать, чтобы стать бэкенд разработчиком?\", top_k=10)\n",
        "\n",
        "for i, h in enumerate(hits, 1):\n",
        "    print(f\"\\n#{i} score={h['score']:.4f} chunk_id={h['payload'].get('chunk_id')}\")\n",
        "    print(h[\"payload\"][\"text\"][:300])\n",
        "\n",
        "\n",
        "def generate_rag_answer(question: str, hits, model: str = \"gpt-4o-mini\") -> str:\n",
        "    \"\"\"Генерация ответа на основе найденных чанков\"\"\"\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"[chunk {h['payload'].get('chunk_id')}] {h['payload']['text']}\"\n",
        "        for h in hits\n",
        "    )\n",
        "\n",
        "    rag_prompt = f\"\"\"Ты карьерный ассистент. Отвечай на вопрос, опираясь на контекст ниже. Выдавай 5-7 навыков из контекста и давай к ним пояснение строго из контекста.\n",
        "Если в контексте нет ответа — честно скажи, что информации недостаточно и предложи 2–3 уточняющих вопроса.\n",
        "\n",
        "Контекст:\n",
        "{context}\n",
        "\n",
        "Вопрос:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "    resp = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
        "        stream=False,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "def answer(question: str, top_k: int = None, model: str = None, score_threshold: float = None) -> str:\n",
        "    \"\"\"Основная функция для получения ответа. Чанки с score ниже score_threshold не передаются в LLM.\"\"\"\n",
        "    top_k = top_k if top_k is not None else config.TOP_K\n",
        "    model = model or config.LLM_MODEL\n",
        "    score_threshold = score_threshold if score_threshold is not None else config.SCORE_THRESHOLD\n",
        "    hits = retrieve(question, top_k=top_k)\n",
        "    if score_threshold > 0:\n",
        "        hits = [h for h in hits if h.get(\"score\", 0) >= score_threshold]\n",
        "    if not hits:\n",
        "        return \"Я не нашёл релевантных знаний в базе. Попробуй переформулировать вопрос.\"\n",
        "    return generate_rag_answer(question, hits, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAKjAW_lOtL_"
      },
      "source": [
        "## Телеграм бот"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d8BsFAlHzb_",
        "outputId": "de0163f9-8c64-4c13-ba59-c4454bf3c5b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тг бот активен\n"
          ]
        }
      ],
      "source": [
        "bot = telebot.TeleBot(config.TELEGRAM_BOT_TOKEN)\n",
        "MAX_LEN = config.MAX_MESSAGE_LEN\n",
        "from rag_logger import setup_bot_logger, log_request, log_error\n",
        "bot_logger = setup_bot_logger(config.LOG_DIR)  \n",
        "\n",
        "def split_message(text, max_len=MAX_LEN):\n",
        "    return [text[i:i+max_len] for i in range(0, len(text), max_len)] if text else [\"\"]\n",
        "\n",
        "@bot.message_handler(commands=[\"start\", \"help\"])\n",
        "def start_help(message):\n",
        "    bot.reply_to(\n",
        "        message,\n",
        "        \"Привет! Напиши вопрос — я отвечу с опорой на базу.\\n\",\n",
        "        parse_mode='Markdown'\n",
        "    )\n",
        "\n",
        "@bot.message_handler(func=lambda m: True, content_types=[\"text\"])\n",
        "def handle_text(message):\n",
        "    q = (message.text or \"\").strip()\n",
        "    if not q:\n",
        "        return\n",
        "    try:\n",
        "        bot.send_chat_action(message.chat.id, \"typing\")\n",
        "        resp = answer(q)\n",
        "        log_request(bot_logger, q, len(resp))\n",
        "        for part in split_message(resp):\n",
        "            bot.reply_to(message, part, parse_mode='Markdown')\n",
        "    except Exception as e:\n",
        "        log_error(bot_logger, q, e)\n",
        "        bot.reply_to(message, f\"Ошибка: {e}\", parse_mode='Markdown')\n",
        "\n",
        "def run_bot():\n",
        "    bot.infinity_polling(skip_pending=True, timeout=60, long_polling_timeout=60)\n",
        "\n",
        "# Запуск бота в отдельном потоке\n",
        "threading.Thread(target=run_bot, daemon=True).start()\n",
        "print(\"Тг бот активен\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAO5UXBdPPU6"
      },
      "source": [
        "## Функции для оценки качества RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rxpJ8WQzHzZb"
      },
      "outputs": [],
      "source": [
        "# Функции для расчета метрик качества\n",
        "def simple_text_similarity(text1: str, text2: str) -> float:\n",
        "    \"\"\"Простая оценка схожести текстов для проверки релевантности\"\"\"\n",
        "    text1_clean = re.sub(r'\\s+', ' ', text1.lower()).strip()\n",
        "    text2_clean = re.sub(r'\\s+', ' ', text2.lower()).strip()\n",
        "\n",
        "    if text1_clean in text2_clean or text2_clean in text1_clean:\n",
        "        return 1.0\n",
        "\n",
        "    words1 = set(text1_clean.split())\n",
        "    words2 = set(text2_clean.split())\n",
        "\n",
        "    if not words1 or not words2:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = words1.intersection(words2)\n",
        "    return len(intersection) / max(len(words1), len(words2))\n",
        "\n",
        "def calculate_context_precision(retrieved_contexts: List[str],\n",
        "                                relevant_contexts: List[str],\n",
        "                                top_k: int = 5) -> float:\n",
        "    \"\"\"Вычисляет Context Precision\"\"\"\n",
        "    if not retrieved_contexts:\n",
        "        return 0.0\n",
        "\n",
        "    k = min(top_k, len(retrieved_contexts))\n",
        "    retrieved = retrieved_contexts[:k]\n",
        "\n",
        "    relevant_retrieved = 0\n",
        "    precision_at_k_sum = 0\n",
        "\n",
        "    for i in range(1, k + 1):\n",
        "        current_context = retrieved[i-1]\n",
        "        is_relevant = False\n",
        "\n",
        "        for ref_context in relevant_contexts:\n",
        "            if simple_text_similarity(current_context, ref_context) > 0.3:\n",
        "                is_relevant = True\n",
        "                break\n",
        "\n",
        "        if is_relevant:\n",
        "            relevant_retrieved += 1\n",
        "\n",
        "        precision_at_i = relevant_retrieved / i\n",
        "\n",
        "        if is_relevant:\n",
        "            precision_at_k_sum += precision_at_i\n",
        "\n",
        "    if relevant_retrieved == 0:\n",
        "        return 0.0\n",
        "\n",
        "    context_precision = precision_at_k_sum / relevant_retrieved\n",
        "    return min(1.0, context_precision)\n",
        "\n",
        "def calculate_context_recall(retrieved_contexts: List[str],\n",
        "                            reference_answer: str) -> float:\n",
        "    \"\"\"Упрощенный расчет Context Recall\"\"\"\n",
        "    statements = []\n",
        "    for sentence in re.split(r'[.,;]', reference_answer):\n",
        "        sentence = sentence.strip()\n",
        "        if len(sentence) > 20:\n",
        "            statements.append(sentence)\n",
        "\n",
        "    if not statements:\n",
        "        return 0.0\n",
        "\n",
        "    supported_statements = 0\n",
        "\n",
        "    for statement in statements:\n",
        "        statement_norm = statement.lower()\n",
        "        statement_words = set(re.findall(r'\\w+', statement_norm))\n",
        "\n",
        "        statement_supported = False\n",
        "\n",
        "        for context in retrieved_contexts:\n",
        "            context_norm = context.lower()\n",
        "            context_words = set(re.findall(r'\\w+', context_norm))\n",
        "\n",
        "            common_words = statement_words.intersection(context_words)\n",
        "            if len(common_words) >= 3:\n",
        "                statement_supported = True\n",
        "                break\n",
        "\n",
        "        if statement_supported:\n",
        "            supported_statements += 1\n",
        "\n",
        "    context_recall = supported_statements / len(statements)\n",
        "    return min(1.0, context_recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLHIlatlQRFn"
      },
      "source": [
        "## Запуск оценки качества"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "908_vRqnTXhD",
        "outputId": "b4e018b1-25fa-4bed-aedc-73e40ef7a07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'result': {'collections': [{'name': 'knowledge_base'}, {'name': 'career_levels_hybrid_v3'}, {'name': 'career_levels'}, {'name': 'career_levels_improved_v1'}, {'name': 'career_levels_bge_v1'}, {'name': 'career_levels_bge_v2'}, {'name': 'career_kb_hybrid'}]}, 'status': 'ok', 'time': 1.248e-05}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "r = requests.get(\n",
        "    f\"{QDRANT_URL.rstrip('/')}/collections\",\n",
        "    headers={\"api-key\": QDRANT_API_KEY}\n",
        ")\n",
        "print(r.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtWEPjzNHzXG",
        "outputId": "d64f9856-1e69-4946-9207-17ef40bae01d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context Precision (средняя): 0.6467\n",
            "Context Recall (средняя):    0.6080\n",
            "Оценено вопросов:           10\n",
            "------------------------------------------------------------\n",
            "\n",
            "1. Вопрос: Какие навыки нужно развивать, чтобы стать бэкенд-разработчиком?...\n",
            "   Context Precision: 0.950\n",
            "   Context Recall:    0.111\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 10\n",
            "\n",
            "2. Вопрос: Что входит в обязанности фронтенд-разработчика?...\n",
            "   Context Precision: 0.833\n",
            "   Context Recall:    0.500\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 6\n",
            "\n",
            "3. Вопрос: Что должен знать и уметь аналитик-разработчик?...\n",
            "   Context Precision: 0.589\n",
            "   Context Recall:    0.600\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 8\n",
            "\n",
            "4. Вопрос: Каковы ключевые аспекты работы ML-разработчика?...\n",
            "   Context Precision: 0.756\n",
            "   Context Recall:    0.333\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 8\n",
            "\n",
            "5. Вопрос: Что входит в сферу ответственности продуктового менеджера?...\n",
            "   Context Precision: 0.833\n",
            "   Context Recall:    0.286\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 9\n",
            "\n",
            "6. Вопрос: Какие навыки необходимы проектному менеджеру?...\n",
            "   Context Precision: 0.000\n",
            "   Context Recall:    0.500\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 10\n",
            "\n",
            "7. Вопрос: Какие языки и технологии использует фронтенд-разработчик?...\n",
            "   Context Precision: 0.806\n",
            "   Context Recall:    0.750\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 4\n",
            "\n",
            "8. Вопрос: Как бэкенд-разработчик обеспечивает безопасность?...\n",
            "   Context Precision: 0.250\n",
            "   Context Recall:    1.000\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 1\n",
            "\n",
            "9. Вопрос: Что включает в себя MLOps у ML-разработчика?...\n",
            "   Context Precision: 1.000\n",
            "   Context Recall:    1.000\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 1\n",
            "\n",
            "10. Вопрос: Как продуктовый менеджер работает с данными?...\n",
            "   Context Precision: 0.450\n",
            "   Context Recall:    1.000\n",
            "   Извлечено контекстов: 5\n",
            "   Релевантных контекстов: 1\n"
          ]
        }
      ],
      "source": [
        "def evaluate_rag_system(test_data: List[Dict], top_k: int = 5) -> Dict[str, Any]:\n",
        "    \"\"\"Полная оценка RAG-системы\"\"\"\n",
        "    all_precisions = []\n",
        "    all_recalls = []\n",
        "    detailed_results = []\n",
        "\n",
        "    for i, item in enumerate(test_data):\n",
        "        question = item[\"question\"]\n",
        "        reference_answer = item[\"reference_answer\"]\n",
        "        reference_contexts = item[\"reference_contexts\"]\n",
        "\n",
        "        hits = retrieve(question, top_k=top_k)\n",
        "\n",
        "        if not hits:\n",
        "            rag_answer = \"\"\n",
        "            retrieved_contexts = []\n",
        "        else:\n",
        "            rag_answer = generate_rag_answer(question, hits)\n",
        "            retrieved_contexts = [h[\"payload\"][\"text\"] for h in hits]\n",
        "\n",
        "        precision = calculate_context_precision(retrieved_contexts, reference_contexts, top_k)\n",
        "        recall = calculate_context_recall(retrieved_contexts, reference_answer)\n",
        "\n",
        "        all_precisions.append(precision)\n",
        "        all_recalls.append(recall)\n",
        "\n",
        "        detailed_results.append({\n",
        "            \"question\": question,\n",
        "            \"rag_answer\": rag_answer[:200] + \"...\" if len(rag_answer) > 200 else rag_answer,\n",
        "            \"reference_answer\": reference_answer[:200] + \"...\" if len(reference_answer) > 200 else reference_answer,\n",
        "            \"retrieved_contexts_count\": len(retrieved_contexts),\n",
        "            \"relevant_contexts_count\": len(reference_contexts),\n",
        "            \"context_precision\": precision,\n",
        "            \"context_recall\": recall,\n",
        "            \"retrieved_contexts\": retrieved_contexts[:2]\n",
        "        })\n",
        "\n",
        "    avg_precision = np.mean(all_precisions) if all_precisions else 0.0\n",
        "    avg_recall = np.mean(all_recalls) if all_recalls else 0.0\n",
        "\n",
        "    return {\"context_precision\": avg_precision,\n",
        "        \"context_recall\": avg_recall,\n",
        "        \"num_questions\": len(test_data),\n",
        "        \"detailed_results\": detailed_results}\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"Запуск оценки RAG-системы\"\"\"\n",
        "    with open('test_dataset.json', 'r', encoding='utf-8') as f:\n",
        "        test_qa_pairs = json.load(f)\n",
        "\n",
        "    results = evaluate_rag_system(test_qa_pairs, top_k=5)\n",
        "\n",
        "    print(f\"Context Precision (средняя): {results['context_precision']:.4f}\")\n",
        "    print(f\"Context Recall (средняя):    {results['context_recall']:.4f}\")\n",
        "    print(f\"Оценено вопросов:           {results['num_questions']}\")\n",
        "    print(\"-\"*60)\n",
        "    #посмотрим по каждому вопросу\n",
        "    for i, detail in enumerate(results['detailed_results']):\n",
        "        print(f\"\\n{i+1}. Вопрос: {detail['question'][:80]}...\")\n",
        "        print(f\"   Context Precision: {detail['context_precision']:.3f}\")\n",
        "        print(f\"   Context Recall:    {detail['context_recall']:.3f}\")\n",
        "        print(f\"   Извлечено контекстов: {detail['retrieved_contexts_count']}\")\n",
        "        print(f\"   Релевантных контекстов: {detail['relevant_contexts_count']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Запуск оценки\n",
        "if __name__ == \"__main__\":\n",
        "    run_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PyMm6DcbLIB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
